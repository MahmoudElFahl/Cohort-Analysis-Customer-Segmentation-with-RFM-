{"cells":[{"metadata":{},"cell_type":"markdown","source":"[](http://)"},{"metadata":{},"cell_type":"markdown","source":"Hi Kagglers,\n\nWelcome to My Second Project. If there are any feedbacks/suggestions you would like to see in the Kernel please let me know. This notebook will always be a work in progress. Please leave any comments about further improvements to the notebook. I appreciate every note!\n\nIf you like it , you can upvote and/or leave a comment :)\n"},{"metadata":{},"cell_type":"markdown","source":"# Cohort Analysis (Retention over User & Product Lifetime)"},{"metadata":{},"cell_type":"markdown","source":"![](http://blog.appsee.com/wp-content/uploads/2018/06/action-cohort-analysis-4.png)"},{"metadata":{},"cell_type":"markdown","source":"![](http://d35fo82fjcw0y8.cloudfront.net/2016/03/03210554/table1a2.png)"},{"metadata":{},"cell_type":"markdown","source":"**A cohort** is a group of subjects who share a defining characteristic. We can observe how a cohort behaves across time and compare it to other cohorts. Cohorts are used in medicine, psychology, econometrics, ecology and many other areas to perform a cross-section (compare difference across subjects) at intervals through time. \n\n**Types of cohorts: \n**\n- Time Cohorts are customers who signed up for a product or service during a particular time frame. Analyzing these cohorts shows the customers’ behavior depending on the time they started using the company’s products or services. The time may be monthly or quarterly even daily.\n- Behaovior cohorts are customers who purchased a product or subscribed to a service in the past. It groups customers by the type of product or service they signed up. Customers who signed up for basic level services might have different needs than those who signed up for advanced services. Understaning the needs of the various cohorts can help a company design custom-made services or products for particular segments.\n- Size cohorts refer to the various sizes of customers who purchase company’s products or services. This categorization can be based on the amount of spending in some periodic time after acquisition or the product type that the customer spent most of their order amount in some period of time.\n"},{"metadata":{},"cell_type":"markdown","source":"**Import Libraries and DataSet **"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import library\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime as dt\n\n#For Data  Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#For Machine Learning Algorithm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\ndf = pd.read_excel('../input/Online Retail.xlsx')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Explore + Clean the data **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Nots that: There are Missing Data in Description and The Customer ID Columns , let's check that**"},{"metadata":{},"cell_type":"markdown","source":"Check and Clean Missing Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.dropna(subset=['CustomerID'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check & Clean Duplicates Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.duplicated().sum()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note That : The min for unit price = 0 and the min for Quantity with negative value "},{"metadata":{"trusted":true},"cell_type":"code","source":"df=df[(df['Quantity']>0) & (df['UnitPrice']>0)]\ndf.describe() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Make Cohort Analysis"},{"metadata":{},"cell_type":"markdown","source":"**For cohort analysis, there are a few labels that we have to create:**\n- Invoice period: A string representation of the year and month of a single transaction/invoice.\n- Cohort group: A string representation of the the year and month of a customer’s first purchase. This label is common across all invoices for a particular customer.\n- Cohort period / Cohort Index: A integer representation a customer’s stage in its “lifetime”. The number represents the number of months passed since the first purchase.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_month(x) : return dt.datetime(x.year,x.month,1)\ndf['InvoiceMonth'] = df['InvoiceDate'].apply(get_month)\ngrouping = df.groupby('CustomerID')['InvoiceMonth']\ndf['CohortMonth'] = grouping.transform('min')\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_month_int (dframe,column):\n    year = dframe[column].dt.year\n    month = dframe[column].dt.month\n    day = dframe[column].dt.day\n    return year, month , day \n\ninvoice_year,invoice_month,_ = get_month_int(df,'InvoiceMonth')\ncohort_year,cohort_month,_ = get_month_int(df,'CohortMonth')\n\nyear_diff = invoice_year - cohort_year \nmonth_diff = invoice_month - cohort_month \n\ndf['CohortIndex'] = year_diff * 12 + month_diff + 1 \ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count monthly active customers from each cohort\ngrouping = df.groupby(['CohortMonth', 'CohortIndex'])\ncohort_data = grouping['CustomerID'].apply(pd.Series.nunique)\n# Return number of unique elements in the object.\ncohort_data = cohort_data.reset_index()\ncohort_counts = cohort_data.pivot(index='CohortMonth',columns='CohortIndex',values='CustomerID')\ncohort_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Retention Rate Table **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Retention table\ncohort_size = cohort_counts.iloc[:,0]\nretention = cohort_counts.divide(cohort_size,axis=0) #axis=0 to ensure the divide along the row axis \nretention.round(3) * 100 #to show the number as percentage ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Build the heatmap\nplt.figure(figsize=(15, 8))\nplt.title('Retention rates')\nsns.heatmap(data=retention,annot = True,fmt = '.0%',vmin = 0.0,vmax = 0.5,cmap=\"BuPu_r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Note That: Customer retention is a very useful metric to understand how many of the all customers are still active.Retention gives you the percentage of active customers compared to the total number of customers.**"},{"metadata":{},"cell_type":"markdown","source":"**Average quantity for each cohort **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Average quantity for each cohort\ngrouping = df.groupby(['CohortMonth', 'CohortIndex'])\ncohort_data = grouping['Quantity'].mean()\ncohort_data = cohort_data.reset_index()\naverage_quantity = cohort_data.pivot(index='CohortMonth',columns='CohortIndex',values='Quantity')\naverage_quantity.round(1)\naverage_quantity.index = average_quantity.index.date\n\n#Build the heatmap\nplt.figure(figsize=(15, 8))\nplt.title('Average quantity for each cohort')\nsns.heatmap(data=average_quantity,annot = True,vmin = 0.0,vmax =20,cmap=\"BuGn_r\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Recency, Frequency and Monetary Value calculation"},{"metadata":{},"cell_type":"markdown","source":"![](http://www.omniconvert.com/blog/wp-content/uploads/2016/03/feature-img-marketizator-rfm.png)"},{"metadata":{},"cell_type":"markdown","source":"**What is RFM?\n**\n- **RFM** is an acronym of recency, frequency and monetary. Recency is about when was the last order of a customer. It means the number of days since a customer made the last purchase. If it’s a case for a website or an app, this could be interpreted as the last visit day or the last login time.\n\n- **Frequency** is about the number of purchase in a given period. It could be 3 months, 6 months or 1 year. So we can understand this value as for how often or how many a customer used the product of a company. The bigger the value is, the more engaged the customers are. Could we say them as our VIP? Not necessary. Cause we also have to think about how much they actually paid for each purchase, which means monetary value.\n\n- **Monetary** is the total amount of money a customer spent in that given period. Therefore big spenders will be differentiated with other customers such as MVP or VIP.\n\n![](http://d35fo82fjcw0y8.cloudfront.net/2018/03/01013508/Incontent_image.png)"},{"metadata":{},"cell_type":"markdown","source":"**The RFM values can be grouped in several ways: **\n\n**1.Percentiles e.g. quantiles **\n\n**2.Pareto 80/20 cut**\n\n**3.Custom - based on business knowledge**\n\n**We are going to implement percentile-based grouping.**"},{"metadata":{},"cell_type":"markdown","source":"**Process of calculating percentiles:**\n1. Sort customers based on that metric\n2. Break customers into a pre-defined number of groups of equal size\n3. Assign a label to each group"},{"metadata":{"trusted":true},"cell_type":"code","source":"#New Total Sum Column  \ndf['TotalSum'] = df['UnitPrice']* df['Quantity']\n\n#Data preparation steps\nprint('Min Invoice Date:',df.InvoiceDate.dt.date.min(),'max Invoice Date:',\n       df.InvoiceDate.dt.date.max())\n\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the real world, we would be working with the most recent snapshot of the data of today or yesterday"},{"metadata":{"trusted":true},"cell_type":"code","source":"snapshot_date = df['InvoiceDate'].max() + dt.timedelta(days=1)\nsnapshot_date\n#The last day of purchase in total is 09 DEC, 2011. To calculate the day periods, \n#let's set one day after the last one,or \n#10 DEC as a snapshot_date. We will cound the diff days with snapshot_date.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate RFM metrics\nrfm = df.groupby(['CustomerID']).agg({'InvoiceDate': lambda x : (snapshot_date - x.max()).days,\n                                      'InvoiceNo':'count','TotalSum': 'sum'})\n#Function Lambdea: it gives the number of days between hypothetical today and the last transaction\n\n#Rename columns\nrfm.rename(columns={'InvoiceDate':'Recency','InvoiceNo':'Frequency','TotalSum':'MonetaryValue'}\n           ,inplace= True)\n\n#Final RFM values\nrfm.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note That : **\n\n**#We will rate \"Recency\" customer who have been active more recently better than the less recent customer,because each company wants its customers to be recent ** \n\n**#We will rate \"Frequency\" and \"Monetary Value\" higher label because we want Customer to spend more money and visit more often(that is  different order than recency). **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building RFM segments\nr_labels =range(4,0,-1)\nf_labels=range(1,5)\nm_labels=range(1,5)\nr_quartiles = pd.qcut(rfm['Recency'], q=4, labels = r_labels)\nf_quartiles = pd.qcut(rfm['Frequency'],q=4, labels = f_labels)\nm_quartiles = pd.qcut(rfm['MonetaryValue'],q=4,labels = m_labels)\nrfm = rfm.assign(R=r_quartiles,F=f_quartiles,M=m_quartiles)\n\n# Build RFM Segment and RFM Score\ndef add_rfm(x) : return str(x['R']) + str(x['F']) + str(x['M'])\nrfm['RFM_Segment'] = rfm.apply(add_rfm,axis=1 )\nrfm['RFM_Score'] = rfm[['R','F','M']].sum(axis=1)\n\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The Result is a Table which has a row for each customer with their RFM **"},{"metadata":{},"cell_type":"markdown","source":"# Analyzing RFM Segments"},{"metadata":{},"cell_type":"markdown","source":"**Largest RFM segments **\n**It is always the best practice to investigate the size of the segments before you use them for targeting or other business Application.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm.groupby(['RFM_Segment']).size().sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Filtering on RFM segments **"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select bottom RFM segment \"111\" and view top 5 rows\nrfm[rfm['RFM_Segment']=='111'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summary metrics per RFM Score **"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm.groupby('RFM_Score').agg({'Recency': 'mean','Frequency': 'mean',\n                             'MonetaryValue': ['mean', 'count'] }).round(1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Use RFM score to group customers into Gold, Silver and Bronze segments:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def segments(df):\n    if df['RFM_Score'] > 9 :\n        return 'Gold'\n    elif (df['RFM_Score'] > 5) and (df['RFM_Score'] <= 9 ):\n        return 'Sliver'\n    else:  \n        return 'Bronze'\n\nrfm['General_Segment'] = rfm.apply(segments,axis=1)\n\nrfm.groupby('General_Segment').agg({'Recency':'mean','Frequency':'mean',\n                                    'MonetaryValue':['mean','count']}).round(1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Pre-Processing for Kmeans Clustering"},{"metadata":{},"cell_type":"markdown","source":"**Key k-means assumptions**\n- Symmetric distribution of variables (not skewed)\n- Variables with same average values\n- Variables with same variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_rfm = rfm[['Recency','Frequency','MonetaryValue']]\nprint(rfm_rfm.describe())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Problem: Mean and Variance not Equal**\n\n**Soluation: Scaling variables by using a scaler from scikit-learn library**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the distribution of RFM values\nf,ax = plt.subplots(figsize=(10, 12))\nplt.subplot(3, 1, 1); sns.distplot(rfm.Recency, label = 'Recency')\nplt.subplot(3, 1, 2); sns.distplot(rfm.Frequency, label = 'Frequency')\nplt.subplot(3, 1, 3); sns.distplot(rfm.MonetaryValue, label = 'Monetary Value')\nplt.style.use('fivethirtyeight')\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Problem: Un Symmetric distribution of variables (skewed)**\n\n**Soluation:Logarithmic transformation (positive values only) will manage skewness**"},{"metadata":{},"cell_type":"markdown","source":"**Sequence of structuring pre-processing steps**\n\n**1. Unskew the data - log transformation **\n\n**2. Standardize to the same average values **\n\n**3. Scale to the same standard deviation **\n\n**4. Store as a separate array to be used for clustering**\n_______________________________\n\n**Why the sequence matters?**\n- Log transformation only works with positive data\n- Normalization forces data to have negative values and log will not work"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unskew the data with log transformation\nrfm_log = rfm[['Recency', 'Frequency', 'MonetaryValue']].apply(np.log, axis = 1).round(3)\n#or rfm_log = np.log(rfm_rfm)\n\n\n# plot the distribution of RFM values\nf,ax = plt.subplots(figsize=(10, 12))\nplt.subplot(3, 1, 1); sns.distplot(rfm_log.Recency, label = 'Recency')\nplt.subplot(3, 1, 2); sns.distplot(rfm_log.Frequency, label = 'Frequency')\nplt.subplot(3, 1, 3); sns.distplot(rfm_log.MonetaryValue, label = 'Monetary Value')\nplt.style.use('fivethirtyeight')\nplt.tight_layout()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementation of K-Means Clustering"},{"metadata":{},"cell_type":"markdown","source":"**Key steps**\n1. Data pre-processing\n2. Choosing a number of clusters\n3. Running k-means clustering on pre-processed data\n4. Analyzing average RFM values of each cluster"},{"metadata":{},"cell_type":"markdown","source":"**** 1. Data Pre-Processing****\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize the variables with StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(rfm_log)\n#Store it separately for clustering\nrfm_normalized= scaler.transform(rfm_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Choosing a Number of Clusters**"},{"metadata":{},"cell_type":"markdown","source":"**Methods to define the number of clusters**\n- Visual methods - elbow criterion\n- Mathematical methods - silhouette coefficient\n- Experimentation and interpretation\n\n**Elbow criterion method ** \n- Plot the number of clusters against within-cluster sum-of-squared-errors (SSE) - sum of squared distances from every data point to their cluster center\n- Identify an \"elbow\" in the plot\n- Elbow - a point representing an \"optimal\" number of clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n#First : Get the Best KMeans \nks = range(1,8)\ninertias=[]\nfor k in ks :\n    # Create a KMeans clusters\n    kc = KMeans(n_clusters=k,random_state=1)\n    kc.fit(rfm_normalized)\n    inertias.append(kc.inertia_)\n\n# Plot ks vs inertias\nf, ax = plt.subplots(figsize=(15, 8))\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.style.use('ggplot')\nplt.title('What is the Best Number for KMeans ?')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note Theat: We Choose No.KMeans = 3 **"},{"metadata":{"trusted":true},"cell_type":"code","source":"# clustering\nkc = KMeans(n_clusters= 3, random_state=1)\nkc.fit(rfm_normalized)\n\n#Create a cluster label column in the original DataFrame\ncluster_labels = kc.labels_\n\n#Calculate average RFM values and size for each cluster:\nrfm_rfm_k3 = rfm_rfm.assign(K_Cluster = cluster_labels)\n\n#Calculate average RFM values and sizes for each cluster:\nrfm_rfm_k3.groupby('K_Cluster').agg({'Recency': 'mean','Frequency': 'mean',\n                                         'MonetaryValue': ['mean', 'count'],}).round(0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Snake plots to understand and compare segments**\n- Market research technique to compare different segments\n- Visual representation of each segment's attributes\n- Need to first normalize data (center & scale)\n- Plot each cluster's average normalized values of each attribute\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_normalized = pd.DataFrame(rfm_normalized,index=rfm_rfm.index,columns=rfm_rfm.columns)\nrfm_normalized['K_Cluster'] = kc.labels_\nrfm_normalized['General_Segment'] = rfm['General_Segment']\nrfm_normalized.reset_index(inplace = True)\n\n#Melt the data into a long format so RFM values and metric names are stored in 1 column each\nrfm_melt = pd.melt(rfm_normalized,id_vars=['CustomerID','General_Segment','K_Cluster'],value_vars=['Recency', 'Frequency', 'MonetaryValue'],\nvar_name='Metric',value_name='Value')\nrfm_melt.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Snake Plot and Heatmap**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 8))\nsns.lineplot(x = 'Metric', y = 'Value', hue = 'General_Segment', data = rfm_melt,ax=ax1)\n\n# a snake plot with K-Means\nsns.lineplot(x = 'Metric', y = 'Value', hue = 'K_Cluster', data = rfm_melt,ax=ax2)\n\nplt.suptitle(\"Snake Plot of RFM\",fontsize=24) #make title fontsize subtitle \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Relative importance of segment attributes **\n- Useful technique to identify relative importance of each segment's attribute\n- Calculate average values of each cluster\n- Calculate average values of population\n- Calculate importance score by dividing them and subtracting 1 (ensures 0 is returned when cluster average equals population average)\n\n**Let’s try again with a heat map. Heat maps are a graphical representation of data where larger values were colored in darker scales and smaller values in lighter scales. We can compare the variance between the groups quite intuitively by colors. **\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The further a ratio is from 0, the more important that attribute is for a segment relative to the total population\ncluster_avg = rfm_rfm_k3.groupby(['K_Cluster']).mean()\npopulation_avg = rfm_rfm.mean()\nrelative_imp = cluster_avg / population_avg - 1\nrelative_imp.round(2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the mean value in total \ntotal_avg = rfm.iloc[:, 0:3].mean()\n# calculate the proportional gap with total mean\ncluster_avg = rfm.groupby('General_Segment').mean().iloc[:, 0:3]\nprop_rfm = cluster_avg/total_avg - 1\nprop_rfm.round(2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap with RFM\nf, (ax1, ax2) = plt.subplots(1,2, figsize=(15, 5))\nsns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='Blues',ax=ax1)\nax1.set(title = \"Heatmap of K-Means\")\n\n# a snake plot with K-Means\nsns.heatmap(prop_rfm, cmap= 'Oranges', fmt= '.2f', annot = True,ax=ax2)\nax2.set(title = \"Heatmap of RFM quantile\")\n\nplt.suptitle(\"Heat Map of RFM\",fontsize=20) #make title fontsize subtitle \n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**You can Updated RFM data by adding Tenure variable : **\n** -Tenure: time since the first transaction ، Defines how long the customer has been with the company**\n\n**Conclusion:  We talked about how to get RFM values from customer purchase data, and we made two kinds of segmentation with RFM quantiles and K-Means clustering methods. **\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Reference**\n- Customer segmentation on DataCamp course by Karolis Urbonas \"Head of Data Science, Amazon\" https://www.datacamp.com/courses/customer-segmentation-in-python\n- https://clevertap.com/blog/rfm-analysis/\n- https://clevertap.com/blog/cohort-analysis/\n- https://towardsdatascience.com/who-is-your-golden-goose-cohort-analysis-50c9de5dbd31\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}